## Quick starts
```bash
$ git clone git@github.com:EleutherAI/sparsify.git  # 1. clone EleutherAI's SAE repo inside our UCL-SAE repo directory. pip installation leads to errors.
$ conda env create -f environment.yml  # 2. create a conda env with required packages
```

## TODOs
- [ ] Update the Wandb config to use the team's wandb dashboard to share the training details
- [ ] Write a new code to make sure that all model-generated datasets have the same number of *tokens*

## Dataset Generation

### Using generate_datasets.py

This script generates datasets using language models like Pythia with unconditional generation.

```bash
python generate_datasets.py [OPTIONS]
```

Currently, the primary parameters of interest are `seed` and `models`:

```bash
python generate_datasets.py --seed 42 --models EleutherAI/pythia-1.4b EleutherAI/pythia-1b
```

All Available Options

```
--models        List of models to use (default: ["EleutherAI/pythia-1.4b"])
--seed          Random seed for generation (default: 42)
--temperature   Temperature for generation (default: 1.0)
--top_p         Top-p for generation (default: 0.9)
--n             Number of samples to generate (default: 400,000)
```

## Sparse Autoencoder (SAE) Training

### Using train_sae.py

This script trains Sparse Autoencoders on activations from language models like Pythia.

```bash
python train_sae.py [OPTIONS]
```

### Key Concepts

- **Target Models**: The models you want to train the SAE on (specified with `--target_models`)
- **Source Models**: The models that generated the datasets (specified with `--source_models`)

The script will train SAEs for each combination of target model and source model dataset.

### Example Usage

Train an SAE on the pythia-1.4b model using a dataset from the same model:
```bash
python train_sae.py --target_models pythia-1.4b --source_models pythia-1.4b
```

Train SAEs on pythia-1.4b using datasets from both pythia-1.4b and pythia-2.8b:
```bash
python train_sae.py --target_models pythia-1.4b --source_models pythia-1.4b pythia-2.8b
```

Cross-model training with specific layers:
```bash
python train_sae.py --target_models pythia-1.4b pythia-2.8b --source_models pythia-1.4b --layers 7 15
```

### All Available Options

```
--org_name             Organization name for model loading (default: "EleutherAI")
--target_models        Models to train SAEs on (default: ["pythia-1.4b", "pythia-2.8b"])
--source_models        Models that generated the datasets (default: ["pythia-1.4b"])
--dataset_seed         Seed used when generating the source datasets (default: 4)
--dataset_temperature  Temperature used when generating the source datasets (default: 1.0)
--dataset_top_p        Top-p used when generating the source datasets (default: 0.9)
--num_seeds            Number of seeds for initializing the SAE weights (default: 2)
--layers               Layers to sparsify in the target models (default: [7, 15])
--project_name         WandB project name (default: "sae-training")
--batch_size           Training batch size (default: 2)
```

### Overview

The `train_sae.py` script performs the following:

1. Loads specified target language models 
2. Uses datasets generated by the specified source models
3. For each target model, trains SAEs using all source model datasets
4. Trains on specified layers of the target models
5. Logs training progress to Weights & Biases
6. Manages experiment organization with different seeds

Different expansion factors are used for different model sizes to match the final SAE dictionary size:
- pythia-1.4b: 20x
- pythia-2.8b: 16x

The script cleans up checkpoint files after training to save disk space.